import os
import shutil
import streamlit as st
import requests
import xml.etree.ElementTree as ET
import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from datetime import datetime

############################################
# Configuration NLTK pour Streamlit Cloud

# D√©finir un dossier local pour stocker les ressources NLTK
nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')
if not os.path.exists(nltk_data_dir):
    os.makedirs(nltk_data_dir)
os.environ['NLTK_DATA'] = nltk_data_dir

def download_nltk_resource(resource_name):
    try:
        nltk.data.find(resource_name)
    except LookupError:
        nltk.download(resource_name, download_dir=nltk_data_dir)

# T√©l√©charger les ressources n√©cessaires
download_nltk_resource('tokenizers/punkt')
download_nltk_resource('corpora/stopwords')

# Pour satisfaire la recherche du dossier "tokenizers/punkt_tab/english",
# nous cr√©ons ce dossier en copiant le fichier "english.pickle" depuis "punkt".
punkt_dir = os.path.join(nltk_data_dir, "tokenizers", "punkt")
punkt_tab_dir = os.path.join(nltk_data_dir, "tokenizers", "punkt_tab")
english_tab_dir = os.path.join(punkt_tab_dir, "english")
if not os.path.exists(english_tab_dir):
    os.makedirs(english_tab_dir, exist_ok=True)
    english_pickle = os.path.join(punkt_dir, "english.pickle")
    if os.path.exists(english_pickle):
        shutil.copy(english_pickle, english_tab_dir)
    else:
        st.warning("Le fichier english.pickle n'a pas √©t√© trouv√© dans le dossier 'punkt'.")

############################################
# Interface Streamlit

st.title("üîé Recherche et Filtrage d'Articles PubMed")

st.write(
    "Cette application vous permet d'extraire des articles de PubMed en fonction d'une requ√™te, "
    "de limiter la recherche √† une p√©riode pr√©cise, et d'agr√©ger les r√©sultats dans un fichier Excel "
    "en ajoutant des informations suppl√©mentaires."
)

# Saisie de la requ√™te PubMed
query_input = st.text_input("Entrez votre requ√™te PubMed :", "cancer AND immunotherapy")

# S√©lection de la p√©riode de publication
date_range = st.date_input(
    "S√©lectionnez la p√©riode de publication (d√©but et fin) :",
    [pd.to_datetime("2000-01-01"), pd.to_datetime("2025-01-01")]
)
if isinstance(date_range, list) and len(date_range) == 2:
    start_date, end_date = date_range
    # Format PubMed : YYYY/MM/DD
    start_str = start_date.strftime("%Y/%m/%d")
    end_str = end_date.strftime("%Y/%m/%d")
    date_query = f" AND ({start_str}[dp] : {end_str}[dp])"
else:
    date_query = ""

# Nombre d'articles √† r√©cup√©rer
max_results = st.slider("Nombre d'articles √† r√©cup√©rer :", 5, 50, 10)

# Mots-cl√©s pour le filtrage des articles
keywords_input = st.text_input("Mots-cl√©s pour le filtrage (s√©par√©s par des virgules) :", "treatment, therapy, trial")
keywords = [kw.strip().lower() for kw in keywords_input.split(",")]

# Requ√™te compl√®te (incluant la p√©riode)
full_query = query_input + date_query
st.markdown(f"**Requ√™te PubMed compl√®te :** `{full_query}`")

############################################
# Fonction pour r√©cup√©rer les articles depuis PubMed via l'API

def get_pubmed_articles(query, max_results):
    search_url = (
        f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"
        f"db=pubmed&term={query}&retmax={max_results}&retmode=xml"
    )
    search_response = requests.get(search_url)
    try:
        search_root = ET.fromstring(search_response.content)
    except Exception as e:
        st.error(f"Erreur lors du parsing de la r√©ponse de recherche : {e}")
        return []
    
    pmid_list = [elem.text for elem in search_root.findall(".//Id")]
    
    articles = []
    # Pour chaque PMID, r√©cup√©rer les d√©tails de l'article
    for pmid in pmid_list:
        fetch_url = (
            f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"
            f"db=pubmed&id={pmid}&retmode=xml"
        )
        fetch_response = requests.get(fetch_url)
        if fetch_response.status_code != 200:
            st.warning(f"Erreur lors de la r√©cup√©ration du PMID {pmid}: statut {fetch_response.status_code}")
            continue
        
        try:
            fetch_root = ET.fromstring(fetch_response.content)
        except Exception as e:
            st.warning(f"Erreur de parsing XML pour le PMID {pmid}: {e}")
            continue
        
        # Extraction des informations
        title_elem = fetch_root.find(".//ArticleTitle")
        title = title_elem.text if title_elem is not None else "N/A"
        
        journal_elem = fetch_root.find(".//Journal/Title")
        journal = journal_elem.text if journal_elem is not None else "N/A"
        
        year_elem = fetch_root.find(".//PubDate/Year")
        year = year_elem.text if year_elem is not None else "N/A"
        
        abstract_elem = fetch_root.find(".//Abstract/AbstractText")
        abstract = abstract_elem.text if abstract_elem is not None else "N/A"
        
        # Lien vers la fiche d√©taill√©e sur PubMed
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
        
        articles.append({
            "PMID": pmid,
            "Title": title,
            "Journal": journal,
            "Year": year,
            "Abstract": abstract,
            "URL": url
        })
    return articles

############################################
# Fonction pour filtrer les articles selon les mots-cl√©s

def filter_articles(articles, keywords):
    filtered = []
    stop_words = set(stopwords.words("english"))
    
    for article in articles:
        # Concat√©ner titre et abstract pour l'analyse
        text = (article["Title"] + " " + article["Abstract"]).lower()
        # Nettoyer le texte : ne conserver que les lettres et espaces
        text = re.sub(r"[^a-z\s]", " ", text)
        tokens = word_tokenize(text)
        tokens = [word for word in tokens if word not in stop_words]
        
        # Calculer un score de pertinence (nombre d'apparitions des mots-cl√©s)
        score = sum(1 for word in tokens if word in keywords)
        if score > 0:
            article["Pertinence"] = score
            filtered.append(article)
    
    # Trier les articles par score d√©croissant
    filtered.sort(key=lambda x: x["Pertinence"], reverse=True)
    return filtered

############################################
# Action lors du clic sur le bouton "Lancer la recherche"

if st.button("Lancer la recherche"):
    with st.spinner("R√©cup√©ration des articles depuis PubMed..."):
        articles = get_pubmed_articles(full_query, max_results)
    filtered_articles = filter_articles(articles, keywords)
    
    if filtered_articles:
        df = pd.DataFrame(filtered_articles)
        st.success(f"Articles pertinents trouv√©s : {len(df)}")
        st.dataframe(df)
        
        # Ajouter trois colonnes suppl√©mentaires :
        # - Date d'ex√©cution de la requ√™te
        # - Contenu de la requ√™te compl√®te
        # - Liste des mots-cl√©s utilis√©s
        execution_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        df["Execution Date"] = execution_date
        df["Query"] = full_query
        df["Keywords"] = keywords_input
        
        # Chemin du fichier Excel dans lequel les r√©sultats seront agr√©g√©s
        output_file = "pubmed_filtered_results.xlsx"
        
        # Si le fichier existe d√©j√†, le charger et concat√©ner les nouveaux r√©sultats
        if os.path.exists(output_file):
            try:
                existing_df = pd.read_excel(output_file)
                combined_df = pd.concat([existing_df, df], ignore_index=True)
            except Exception as e:
                st.warning(f"Erreur lors du chargement du fichier Excel existant : {e}")
                combined_df = df
        else:
            combined_df = df
        
        # Sauvegarder le DataFrame combin√© dans le fichier Excel
        combined_df.to_excel(output_file, index=False)
        
        # Proposer le t√©l√©chargement du fichier Excel
        with open(output_file, "rb") as file:
            st.download_button(
                label="T√©l√©charger le fichier Excel",
                data=file,
                file_name=output_file,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    else:
        st.warning("Aucun article pertinent n'a √©t√© trouv√© avec les crit√®res d√©finis.")
